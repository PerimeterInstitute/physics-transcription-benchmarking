{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# luis to disable file caching\n",
    "# from os import environ\n",
    "# environ['PYTHONDONTWRITEBYTECODE'] = 'None'\n",
    "\n",
    "from Test import Test\n",
    "from models.WhisperOpenAI import WhisperOpenAI\n",
    "from models.WhisperPI import WhisperPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = WhisperOpenAI(\"whisper_openai_medium\",\n",
    "                  {\n",
    "                    \"model_type\": \"medium\",\n",
    "                    \"language\": \"en\"\n",
    "                  }\n",
    ")\n",
    "\n",
    "model_2 = WhisperPI(\"whisper_pi_medium\",\n",
    "                  {\n",
    "                    \"model_type\": \"medium\",\n",
    "                    \"language\": \"en\"\n",
    "                  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/pi_whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/pi_whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/lrivas/anaconda3/envs/whisper_env/lib/python3.11/site-packages/pi_whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "test = Test([model_1, model_2], dataset_path=\"../datasets/dev_dataset/\") # TODO: add prompt example?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Test' object has no attribute 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mresults)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Test' object has no attribute 'results'"
     ]
    }
   ],
   "source": [
    "print(test.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fitting_models_with_mcmc_talk': \" okay, so why we often need Markov Chain Monte Carlo is that in real life these models and likelihoods are often pretty complicated. So the constraints are kind of complicated and we can't really represent them with in analytic form, but you can represent them by drawing samples from the distribution and MCMC is a way of drawing samples from a probability distribution when you can compute it numerically but you can't represent it analytically or can't solve it analytically.\", 'celestial_holography_talk': \" Let's start with initial data on past null infinity. Then there's some final data for my outstate at future null infinity. I can think about how to expand my metric or look at the data of the metric near those boundaries. What matters for me right now for the rest of this talk is essentially I'll be holding U fixed and taking large R in some of my expressions. If you were just thinking about your Newtonian gravity, you already expect the metric isn't going to be flat anymore once you add some matter.\", 'computing_entanglement_talk': \" Quantum Monte Carlo are similar scaling methods. Of course, DMRG is the one that we have in one dimension. The idea that we can calculate von Neumann entanglement entropy in DMRG is well known. We have direct access to the reduced density matrix there. In quantum Monte Carlo, maybe this isn't so obvious. Like I said, Max did all the hard work. The reason I'm talking about this in the context of condensed matter, I guess, is that anything we can simulate with quantum Monte Carlo, any condensed matter type system, we hope to be able to access really sub-leading corrections to the area law, if you will, as a resource for identifying new phases, universality at phase transitions. A lot of the motivation behind this came out of the desire to quantify spin liquids with this topological entanglement entropy. I guess not all the scaling of the entropy is the area law. As Max mentioned, free fermions. I think these spin-bose metals that Matthew will hopefully talk about will have a multiplicative log correction. There you can imagine that if we could find a spin-bose metal in a Hamiltonian that we can simulate with quantum Monte Carlo, which isn't trivial in any sense, then we can look for essentially the scaling of the entanglement entropy to identify these types of phases. So quantum Monte Carlo, one of our tools of choice here. Really the advantage in quantum Monte Carlo is that it's scalable in system size. Modern methods scale as n, the number of lattice sites. Some methods that we use also scales n squared. We consider this poor scaling for quantum Monte Carlo. N squared simulations can typically be improved upon with clever tricks. You might be familiar with this, the determinantal quantum Monte Carlo, if you're used to looking at fermionic Hubbard models, for example, in two dimensions.\"}\n",
      "{'fitting_models_with_mcmc_talk': '0:00:51.875404', 'celestial_holography_talk': '0:00:58.354684', 'computing_entanglement_talk': '0:02:46.149141'}\n"
     ]
    }
   ],
   "source": [
    "print(model_1.transcription)\n",
    "print(model_1.transcribe_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
