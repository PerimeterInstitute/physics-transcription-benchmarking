{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Test import Test\n",
    "from models.WhisperOpenAI import WhisperOpenAI\n",
    "from models.WhisperPI import WhisperPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = WhisperOpenAI(\"whisper_openai_medium\",\n",
    "                  {\n",
    "                    \"model_type\": \"medium\",\n",
    "                    \"language\": \"en\"\n",
    "                  }\n",
    ")\n",
    "\n",
    "model_2 = WhisperPI(\"whisper_pi_medium\",\n",
    "                  {\n",
    "                    \"model_type\": \"medium\",\n",
    "                    \"language\": \"en\"\n",
    "                  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmohl/.local/lib/python3.8/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/rmohl/.local/lib/python3.8/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/rmohl/.local/lib/python3.8/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/rmohl/.local/lib/python3.8/site-packages/pi_whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/rmohl/.local/lib/python3.8/site-packages/pi_whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/rmohl/.local/lib/python3.8/site-packages/pi_whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "test = Test(model_array=[model_1, model_2])\n",
    "\n",
    "test.run(run_name=\"basic_test\",\n",
    "         dataset_path=\"datasets/dev_dataset/\",\n",
    "         run_num=1)\n",
    "\n",
    "test.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper OpenAI transcriptions:\n",
      "{'fitting_models_with_mcmc_talk': \" Okay, so why we often need Markov chain Monte Carlo is that in real life these models and\\n likelihoods are often pretty complicated.\\n So the constraints are kind of complicated and we can't really represent them with in\\n analytic form.\\n But you can represent them by drawing samples from the distribution.\\n And MCMC is a way of drawing samples from a probability distribution when you can compute\\n it numerically but you can't represent it analytically or can't solve it analytically.\\n\", 'celestial_holography_talk': \" naturally starting with initial data on past null infinity, and then there's some final\\n data for my out state at future null infinity. And so I can think about how to like expand\\n my metric or look at the data of the metric near those boundaries. And so what matters\\n for me right now for the rest of this talk is essentially I'll be holding U fixed and\\n taking large R in some of my expressions. So if you were just thinking about your Newtonian\\n gravity, you already expect the metric isn't going to be flat anymore once you add some\\n matter. So for essence.\\n\", 'computing_entanglement_talk': \" so quantum on the carlo are similar\\n uh... scaling methods of course the images one that we have a one-dimensional\\n right and so the idea\\n you know the idea that\\n you can calculate\\n bono i'm going to enter the indian margie's well-known we've direct access\\n to the reduced density matrix there\\n so i want to call me this isn't so obvious\\n and so i think that all the hard work and the reason um...\\n i'm talking about this in the context of condensed matter\\n i guess is that anything we can simulate with quantum on the makan lo any\\n that's matter type system\\n uh... we hope to be able to access\\n really sub leading corrections to the area law\\n if you will\\n you know as a resource for identifying new phases\\n you know universality at phase transitions\\n a lot of this came out of\\n a lot of motivation behind this came out of the\\n uh... desire to\\n sort of quantify spin liquids with the stop logical entanglement entropy\\n i guess not all that all not all the sub leading corrections\\n okay not not all the\\n all the scaling of the entropy is the area law\\n as much mention free for me on to think he's been both metals\\n that matthew will will be talked about will have a\\n multiplicative log correction\\n okay so they're going to match and that if we could\\n if we can find a spin both metal\\n and you know in a hamiltonian again similarly quantum on the carlo which\\n isn't which is in trivial\\n in any sense then we can look for\\n essentially the scaling of the entanglement entropy\\n identify these types of phases\\n so quite a lot of carlo\\n so one of our tools of choice here\\n uh... really the advantage in quantum on the carlo is that it's scalable in\\n system size\\n some modern methods scale is and the number of\\n lattice sites\\n and some\\n methods that we use also scales and squared\\n we consider this for scaling for\\n for uh... quantum on the carlo\\n and squared simulations can typically be improved upon with clever tricks\\n you know you might be familiar with this that determined\\n terminal quantum on the carlo if you're used to looking at\\n uh...\\n for me on a cover models for example in two dimensions\\n\"}\n",
      "{'fitting_models_with_mcmc_talk': '0:00:15.535111', 'celestial_holography_talk': '0:00:16.189080', 'computing_entanglement_talk': '0:01:15.545054'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Whisper OpenAI transcriptions:\")\n",
    "print(model_1.transcription)\n",
    "print(model_1.transcribe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper PI transcriptions:\n",
      "{'fitting_models_with_mcmc_talk': \" Okay, so why we often need Markov chain Monte Carlo is that in real life these models and\\n likelihoods are often pretty complicated.\\n So the constraints are kind of complicated and we can't really represent them with in\\n analytic form.\\n But you can represent them by drawing samples from the distribution.\\n And MCMC is a way of drawing samples from a probability distribution when you can compute\\n it numerically but you can't represent it analytically or can't solve it analytically.\\n\", 'celestial_holography_talk': \" naturally starting with initial data on past null infinity, and then there's some final\\n data for my out state at future null infinity. And so I can think about how to like expand\\n my metric or look at the data of the metric near those boundaries. And so what matters\\n for me right now for the rest of this talk is essentially I'll be holding U fixed and\\n taking large R in some of my expressions. So if you were just thinking about your Newtonian\\n gravity, you already expect the metric isn't going to be flat anymore once you add some\\n matter. So for essence\\n\", 'computing_entanglement_talk': \" so quantum on a carlo are similar\\n uh... scaling methods of course the images one that we have a one-dimensional\\n right and so the idea\\n you know the idea that\\n you can calculate\\n bono and entanglement entropy and dmrg is well known we have direct access to\\n the reduced density matrix there\\n so quantum on a carlo maybe this isn't so obvious\\n and so i can say max did all the hard work and the reason um...\\n i'm talking about this in the context of condensed matter\\n i guess is that anything we can simulate with quantum on a carlo any condensed\\n matter type system\\n uh... we hope to be able to access\\n really sub leading corrections to the area law\\n if you will\\n you know as a resource for identifying new phases\\n you know universality at phase transitions\\n a lot of this came out of\\n a lot of motivation behind this came out of the\\n uh... desire to\\n sort of quantify spin liquids with this topological entanglement entropy\\n i guess not all the\\n all not all the sub leading corrections\\n okay not not all the\\n all the scaling of the entropy is the area law\\n as max mentioned free fermions i think the spin bolts metals\\n that matthew will hopefully talk about will have a\\n multiplicative log correction\\n okay so there you can imagine that if we could\\n if we could find a spin bolts metal\\n in a hamiltonian that we can simulate with quantum on a carlo which isn't\\n which isn't trivial\\n in any sense then we can look for\\n essentially the scaling of the entanglement entropy\\n identify these types of phases\\n so on my carlo so one of our tools of choice here\\n uh... really the advantage in quantum on a carlo is that it's scalable and system\\n size\\n some modern methods scale is and the number of\\n lattice sites\\n and some\\n methods that we use also scales and squared\\n we consider this for scaling for\\n for uh... quantum on a carlo\\n and squared simulations can typically be improved upon with clever tricks\\n you know you might be familiar with this that determined\\n the terminal quantum on a carlo if you're used to looking at\\n uh... uh...\\n for me on a cover models for example in two dimensions\\n\"}\n",
      "{'fitting_models_with_mcmc_talk': '0:00:16.933384', 'celestial_holography_talk': '0:00:15.709072', 'computing_entanglement_talk': '0:02:06.729912'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Whisper PI transcriptions:\")\n",
    "print(model_2.transcription)\n",
    "print(model_2.transcribe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See 'results-basic_test/' folder for results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
