{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_classes.Test import Test\n",
    "from models.WhisperCPP import WhisperCPP\n",
    "from models.Parakeet import Parakeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = WhisperCPP(\"whisper_CPP\", \"whisper.cpp/\", {})\n",
    "\n",
    "model_2 = Parakeet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ggml model large-v2 from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
      "Model large-v2 already exists. Skipping download.\n",
      "cmake -B build \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/hkhan/physics-transcription-benchmarking/whisper.cpp/build\n",
      "cmake --build build --config Release\n",
      "gmake[1]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[2]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target main\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target bench\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[  3%] Built target main\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml-base\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[  7%] Built target bench\n",
      "[ 25%] Built target ggml-base\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml-cpu\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[ 54%] Built target ggml-cpu\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[ 58%] Built target ggml\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target whisper\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[ 62%] Built target whisper\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target whisper-bench\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target common\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[ 66%] Built target whisper-bench\n",
      "[ 76%] Built target common\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Entering directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target test-vad-full\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target test-vad\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target vad-speech-segments\u001b[0m\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target whisper-cli\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target whisper-server\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target quantize\u001b[0m\n",
      "gmake[3]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "[ 82%] Built target test-vad-full\n",
      "[ 86%] Built target whisper-cli\n",
      "[ 90%] Built target test-vad\n",
      "[ 92%] Built target vad-speech-segments\n",
      "[100%] Built target quantize\n",
      "[100%] Built target whisper-server\n",
      "gmake[2]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "gmake[1]: Leaving directory '/home/hkhan/physics-transcription-benchmarking/whisper.cpp/build'\n",
      "this is the current audio file path: /home/hkhan/physics-transcription-benchmarking/datasets/dev_dataset/test_data/fitting_models_with_mcmc_talk-30sec.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-large-v2.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 1\n",
      "whisper_init_with_params_no_state: backends   = 1\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5 (large)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:          CPU total size =  3093.99 MB\n",
      "whisper_model_load: model size    = 3093.99 MB\n",
      "whisper_backend_init_gpu: no GPU found\n",
      "whisper_init_state: kv self size  =   83.89 MB\n",
      "whisper_init_state: kv cross size =  251.66 MB\n",
      "whisper_init_state: kv pad  size  =    7.86 MB\n",
      "whisper_init_state: compute buffer (conv)   =   34.69 MB\n",
      "whisper_init_state: compute buffer (encode) =  212.29 MB\n",
      "whisper_init_state: compute buffer (cross)  =    9.25 MB\n",
      "whisper_init_state: compute buffer (decode) =   99.10 MB\n",
      "\n",
      "system_info: n_threads = 4 / 48 | WHISPER : COREML = 0 | OPENVINO = 0 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "\n",
      "main: processing '/home/hkhan/physics-transcription-benchmarking/datasets/dev_dataset/test_data/fitting_models_with_mcmc_talk-30sec.wav' (480256 samples, 30.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.880]   Okay, so why we often need Markov chain Monte Carlo is that in real life these models and\n",
      "[00:00:04.880 --> 00:00:09.240]   likelihoods are often pretty complicated.\n",
      "[00:00:09.240 --> 00:00:13.520]   So the constraints are kind of complicated and we can't really represent them with in\n",
      "[00:00:13.520 --> 00:00:16.040]   analytic form.\n",
      "[00:00:16.040 --> 00:00:21.640]   But you can represent them by drawing samples from the distribution and MCMC is a way of\n",
      "[00:00:21.640 --> 00:00:27.080]   drawing samples from a probability distribution when you can compute it numerically but you\n",
      "[00:00:27.080 --> 00:00:29.920]   can't represent it analytically or can't solve it analytically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/home/hkhan/physics-transcription-benchmarking/TEMP_DATA/basic_test-2/fitting_models_with_mcmc_talk.txt'\n",
      "output_vtt: saving output to '/home/hkhan/physics-transcription-benchmarking/TEMP_DATA/basic_test-2/fitting_models_with_mcmc_talk.vtt'\n",
      "\n",
      "whisper_print_timings:     load time = 26785.42 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    39.35 ms\n",
      "whisper_print_timings:   sample time =   438.05 ms /   555 runs (     0.79 ms per run)\n",
      "whisper_print_timings:   encode time = 87521.57 ms /     2 runs ( 43760.79 ms per run)\n",
      "whisper_print_timings:   decode time =    98.36 ms /     1 runs (    98.36 ms per run)\n",
      "whisper_print_timings:   batchd time = 26542.68 ms /   550 runs (    48.26 ms per run)\n",
      "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (     0.00 ms per run)\n",
      "whisper_print_timings:    total time = 142210.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the current audio file path: /home/hkhan/physics-transcription-benchmarking/datasets/dev_dataset/test_data/quantum_foundations_lecture-2min.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-large-v2.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 1\n",
      "whisper_init_with_params_no_state: backends   = 1\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5 (large)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:          CPU total size =  3093.99 MB\n",
      "whisper_model_load: model size    = 3093.99 MB\n",
      "whisper_backend_init_gpu: no GPU found\n",
      "whisper_init_state: kv self size  =   83.89 MB\n",
      "whisper_init_state: kv cross size =  251.66 MB\n",
      "whisper_init_state: kv pad  size  =    7.86 MB\n",
      "whisper_init_state: compute buffer (conv)   =   34.69 MB\n",
      "whisper_init_state: compute buffer (encode) =  212.29 MB\n",
      "whisper_init_state: compute buffer (cross)  =    9.25 MB\n",
      "whisper_init_state: compute buffer (decode) =   99.10 MB\n",
      "\n",
      "system_info: n_threads = 4 / 48 | WHISPER : COREML = 0 | OPENVINO = 0 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "\n",
      "main: processing '/home/hkhan/physics-transcription-benchmarking/datasets/dev_dataset/test_data/quantum_foundations_lecture-2min.wav' (1920000 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:05.400]   Okay, any pure state can be written like that in some basis.\n",
      "[00:00:05.400 --> 00:00:10.520]   And this provides you with, these are complex numbers, so you have two n complex numbers,\n",
      "[00:00:10.520 --> 00:00:11.520]   okay.\n",
      "[00:00:11.520 --> 00:00:17.040]   That's the two n, but then you need to subtract one because of the overall phase, and another\n",
      "[00:00:17.040 --> 00:00:23.800]   one because we want to impose normalization.\n",
      "[00:00:23.800 --> 00:00:25.880]   Okay.\n",
      "[00:00:25.880 --> 00:00:36.520]   So a lot of our intuition comes from the special case n equal to 2, you know, a qubit.\n",
      "[00:00:36.520 --> 00:00:41.480]   And there's something bad about this special case, we're trying to get an intuition for\n",
      "[00:00:41.480 --> 00:00:43.080]   the shape of state space.\n",
      "[00:00:43.080 --> 00:00:52.320]   Let's see, so if n equals 2, k minus 2, which is the -- well, let me just do all of it.\n",
      "[00:00:52.320 --> 00:01:03.960]   So the -- that means k equals 4, k minus 1, which is the -- I can just do this here actually.\n",
      "[00:01:03.960 --> 00:01:15.360]   So this one, k minus 1 equals 3, you can check my arithmetic.\n",
      "[00:01:15.360 --> 00:01:22.480]   k minus 2 equals 2.\n",
      "[00:01:22.480 --> 00:01:28.200]   So this is all consistent because we know that we have the block ball, and the boundary\n",
      "[00:01:28.200 --> 00:01:31.880]   of the block ball has two dimensions in it.\n",
      "[00:01:31.880 --> 00:01:45.160]   And then also, if you look at this here, you get a pure dimension that's 2n minus 2 is\n",
      "[00:01:45.160 --> 00:01:47.600]   also 2.\n",
      "[00:01:47.600 --> 00:01:49.520]   Okay.\n",
      "[00:01:49.520 --> 00:01:54.280]   So in the case of the qubit, these two things are the same.\n",
      "[00:01:54.280 --> 00:01:55.280]   Okay.\n",
      "[00:01:55.280 --> 00:01:57.560]   So that isn't true usually.\n",
      "[00:01:57.560 --> 00:01:59.240]   Let's take n equal to 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/home/hkhan/physics-transcription-benchmarking/TEMP_DATA/basic_test-2/quantum_foundations_lecture.txt'\n",
      "output_vtt: saving output to '/home/hkhan/physics-transcription-benchmarking/TEMP_DATA/basic_test-2/quantum_foundations_lecture.vtt'\n",
      "\n",
      "whisper_print_timings:     load time =  1888.04 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =   151.21 ms\n",
      "whisper_print_timings:   sample time =  1336.16 ms /  1648 runs (     0.81 ms per run)\n",
      "whisper_print_timings:   encode time = 216250.77 ms /     5 runs ( 43250.15 ms per run)\n",
      "whisper_print_timings:   decode time =    81.29 ms /     1 runs (    81.29 ms per run)\n",
      "whisper_print_timings:   batchd time = 68382.06 ms /  1625 runs (    42.08 ms per run)\n",
      "whisper_print_timings:   prompt time = 23291.27 ms /   694 runs (    33.56 ms per run)\n",
      "whisper_print_timings:    total time = 312846.44 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 09:41:43 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 09:41:44 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-07-14 09:41:44 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 09:41:44 features:305] PADDING: 0\n",
      "[NeMo I 2025-07-14 09:41:49 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-07-14 09:41:49 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 09:41:50 tdt_loop_labels_computer:300] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 09:41:50 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 09:41:51 tdt_loop_labels_computer:300] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 09:41:52 save_restore_connector:275] Model EncDecRNNTBPEModel was successfully restored from /home/hkhan/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v2/snapshots/d97f7ac5d85e7185b7a7c4771c883c0e26d1d16f/parakeet-tdt-0.6b-v2.nemo.\n",
      "[NeMo I 2025-07-14 09:41:52 rnnt_models:291] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n",
      "[NeMo I 2025-07-14 09:41:52 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 09:41:52 tdt_loop_labels_computer:287] `include_duration` is not implemented for CUDA graphs\n",
      "Transcribing: 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 09:41:57 rnnt_models:291] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n",
      "[NeMo I 2025-07-14 09:41:57 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 09:41:57 tdt_loop_labels_computer:287] `include_duration` is not implemented for CUDA graphs\n",
      "Transcribing: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it]\n"
     ]
    }
   ],
   "source": [
    "test = Test(model_array=[model_1,model_2])\n",
    "\n",
    "test.run(run_name=\"basic_test\",\n",
    "         dataset_path=\"datasets/dev_dataset/\",\n",
    "         run_num=1)\n",
    "\n",
    "test.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper OpenAI transcriptions:\n",
      "{'fitting_models_with_mcmc_talk': \" Okay, so why we often need Markov chain Monte Carlo is that in real life these models and\\n likelihoods are often pretty complicated.\\n So the constraints are kind of complicated and we can't really represent them with in\\n analytic form.\\n But you can represent them by drawing samples from the distribution and MCMC is a way of\\n drawing samples from a probability distribution when you can compute it numerically but you\\n can't represent it analytically or can't solve it analytically.\\n\", 'quantum_foundations_lecture': \" Okay, any pure state can be written like that in some basis.\\n And this provides you with, these are complex numbers, so you have two n complex numbers,\\n okay.\\n That's the two n, but then you need to subtract one because of the overall phase, and another\\n one because we want to impose normalization.\\n Okay.\\n So a lot of our intuition comes from the special case n equal to 2, you know, a qubit.\\n And there's something bad about this special case, we're trying to get an intuition for\\n the shape of state space.\\n Let's see, so if n equals 2, k minus 2, which is the -- well, let me just do all of it.\\n So the -- that means k equals 4, k minus 1, which is the -- I can just do this here actually.\\n So this one, k minus 1 equals 3, you can check my arithmetic.\\n k minus 2 equals 2.\\n So this is all consistent because we know that we have the block ball, and the boundary\\n of the block ball has two dimensions in it.\\n And then also, if you look at this here, you get a pure dimension that's 2n minus 2 is\\n also 2.\\n Okay.\\n So in the case of the qubit, these two things are the same.\\n Okay.\\n So that isn't true usually.\\n Let's take n equal to 3.\\n\"}\n",
      "{'fitting_models_with_mcmc_talk': '0:01:49.076124', 'quantum_foundations_lecture': '0:04:57.102555'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Whisper OpenAI transcriptions:\")\n",
    "print(model_1.transcription)\n",
    "print(model_1.transcribe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper PI transcriptions:\n",
      "{'fitting_models_with_mcmc_talk': \"Okay, so why we often need Markov chain Monte Carlo is that in real life these models and likelihoods are often pretty complicated. So the constraints are kind of complicated and we can't really represent them with in analytic form. But you can represent them by drawing samples from the distribution and MCMC is a way of drawing samples from a probability distribution when you can compute it numerically but you can't represent it analytically or can't solve it analytically.\", 'quantum_foundations_lecture': \"Okay, any pure state can be written like that in some basis. And this provides you with, these are complex numbers, so you have two n complex numbers. Okay, that's two n, but then you need to subtract one because of the overall phase, and another one because we want to impose normalization. Okay. So a lot of our intuition comes from the special case n equal to two, you know, qubit. And there's something bad about this special case if we're trying to get an intuition for the shape of state space. So let's see. So if n equals two, k minus two, which is the, well, let me just do all of it. So that means k equals four, k minus one, which is the, I can just do this here actually. Let's do it. Yeah, so this one, k minus one equals three. You can check my arithmetic. k minus two equals two. So this is all consistent because we know that we have the block ball and the boundary of the block ball has two dimensions in it. And then also, if we look at this here, we get a pure dimension. That's 2n minus 2 is also 2. Okay. So in the case of a qubit, these two things are the same. Okay. So that isn't true usually. Let's take n equals 3.\"}\n",
      "{'fitting_models_with_mcmc_talk': '0:00:04.023124', 'quantum_foundations_lecture': '0:00:08.498811'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Whisper PI transcriptions:\")\n",
    "print(model_2.transcription)\n",
    "print(model_2.transcribe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See 'results-basic_test/' folder for results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
