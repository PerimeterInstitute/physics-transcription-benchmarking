It does, okay. So, every object in Julia has a particular type.
For example, the type of A is a double position matrix.
It's not just the matrix, it's a double position matrix. If you look to probably the second lecture,
I spoke about types and generic programming. Let's just actually look at the result here.
The compiler knows that A is a type matrix of float64.
And that means, the compiler knows that every element of that matrix is a float64 number.
So, when it calls the maximum function, which it knows, it knows that it will return a float64 number as well.
Because the maximum of a set of float64 numbers is again a float64 number.
And thus, this allocation goes away, but the other two allocations from the matrix multiplication and from the scaling, they do not go away.
And here we optimize this for the matrix multiplication. We call this low level function from the linear algebra package.
Where we pre-allocated the result once. And here we use a different syntax.
We write A, you write the dot division and dot equals, and then the compiler fuses that into a single loop.
It just sets each element of A by performing this calculation element-wise.
So, if you were to write this out, this would be one loop over all the elements,
and then the respective element of A is set to the respective element of B,
divided by the maximum, which is calculated once ahead of time.
And that is then, that is then this result, which is now fully optimized.
One thing that's weird is, oh never mind.
Okay, so if you look at the Julia documentation, there's a particular section of optimization.
And we will certainly discuss what I spoke about, that you put things into function, even in a notebook,
putting things into function can help improvement.
When you look at the linear algebra section, there is a section, advanced section for experts,
where they also discuss these things. And if you remember how we solved the wave equation,
we pre-allocated that array U for the solution before we used it.
That was just anticipating this part. And there are a few other things that you need to remember when you program in Julia.
Okay, actually let's just profile this again to see what happens here.
Maybe the profile looks better now.
Oh, that was not good. I think by default, it just adds to the previous profile. So, let's clear it first.
And let's look at the result. And it looks much cleaner now. There are many fewer things.
There is, let's look at the total time.
It's now only 3000 ticks. That's to be expected because it's twice as fast.
Actually calculated, the maximum has not become much faster.
And there is the matrix multiplication.
The matrix multiplication itself also takes about the same amount of time. Is it faster?
I think it was 2000 before it became a bit faster. I do not know why.
Oh no, I do understand why it is faster. Because we use the same amount of memory.
The CPU will now cache the memory.
And it is faster to access the same memory multiple times than to access different amounts of memory.
That has to do with the hardware architecture.
Yes?
For me, the allocation is 8.4 kilobytes instead of 7 point something.
Do you have a different matrix size? Is it 30 by 30 or different?
It's 30 by 30. It's medium.
Okay, can you run it again? Maybe it changes slightly.
Okay, I don't understand. You're using symmetry as well?
Okay, I'm using Julia 185. Maybe you're using Julia 184. Maybe there was a change?
Anyway, that's not a big change. That's okay.
Erik, are you using 1.8.4? Yes. So, how much was your allocation for the past version?
One allocation.
If I don't, let me see.
Oh yeah, if you don't have the semicolon of course.
Printing an A. Yes.
Actually, no. It still says one allocation. I don't know why.
Well, the printing happens after the time statement.
Ah, okay.
The time statement returns the result and then the Julia prints it.
Okay, so this concludes the first part of the lecture, where I tell you some basics about Julia.
And that when you have a slow code, then the rules of thumb is that you need to pre-allocate memory,
put things into functions and do a few other things.
But the basic generic rule, independent of Julia, is that if you have a code and you want to make it go fast,
Don't do it. Don't do it yet. Profile before you do it.
And you will not do that. I didn't do that either when I was a student.
I regretted it and I learned it. Now I'm trying to teach you, but there it is.
Okay, next comes a bit of a theoretical section.
Because we're talking about computers, I want to show you how computers work internally.
What the high level layout is, so that you have a bit of an idea of what it means to run multiple threads,
multiple processes, GPU, CPU, distributed computing, etc.
So, this is now independent of the notebook that we're doing here.
So, let's assume you have a computer, say, such as like Symmetry.
And Symmetry consists of several things that are called nodes.
So, let's call it node. And the node is essentially a workstation.
So, a laptop is essentially a node. And internally, there is not just one processor in there,
but the internal layout is a bit more complicated. And there are many different levels,
but I just want to introduce two. I want to introduce sockets and cores.
So, in symmetry, if you look on one of the modern AMD nodes that we have,
They would have four sockets, or thereabouts. I'll just assume it's four.
So, let's draw these in here. And this would be one socket.
And it's called a socket because earlier there was a thing on the motherboard and you click the CPU processor in.
But these days socket means something different. It's just a leftover name for something.
So, four sockets, and each of these sockets has cores. The cores are the things that run the actual code.
Each core can act independently, can independently execute something.
So, this notebook runs on one core, and each of you running your own notebooks, it would use one core to run the actual calculation.
And here on Symmetry, there would say be eight cores in a socket.
So, let's just draw them. 1, 2, 3, 4, 5, 6, 7, 8, what we call the core.
And the memory is attached to individual sockets. That's why I introduced sockets.
So, here we have a memory.
And there's another memory here.
And similar here, memory, memory, and there's a connection here between the socket and the memory.
And the sockets are also connected internally via some kind of high performance network.
I don't know what the AMD version is called, but let's just assume we have something here, and here, and here, and here.
And let's assume there's no direct connection here.
And of course, all these sockets have cores. So, here we have another set of eight cores.
And if this core wants to access this memory, there's a direct connection here. That happens fast.
And if this socket wants to access that memory, it's a bit slower because the information needs to go by multiple network apps.
And it's even a bit slower if you go here, then you go here. That's called NUMA, Non-Uniform Memory Architecture.
Ignore that if you haven't heard about it before. The point is that there's not just one memory,
there are multiple memory subsystems and there are many cores in there.
Now, when you run a calculation, that calculation will of course need to do some number crunching,
adding, multiplying numbers and so on. And that's handled by a core.
But it will also need to access memory. Each core has a bit of cache, a few kilobytes.
So, if it's just a small thing, this matrix here, that probably can sit within a core.
But if it's a large matrix, a thousand by a thousand or something, it will actually be held in memory.
